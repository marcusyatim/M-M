{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "transformer.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7-PtoWFAnK2"
      },
      "source": [
        "# Transformer for Tags\n",
        "### To built a transformer using the 'steps' and 'tags' column of the RAW_recipes.csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e58VeL8V7IH9",
        "outputId": "f09ad55d-def5-4c65-bdac-3a0828e9ba00"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "liCVab5A7Vt4"
      },
      "source": [
        "# Change working directory to be current folder\n",
        "import os\n",
        "os.chdir('/content/gdrive/My Drive/Colab Notebooks/')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFUgd-Bh60UV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea2d0ed9-8440-4c8c-ca38-b148c8231ff6"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import re\n",
        "import string\n",
        "import tensorflow as tf\n",
        "import time\n",
        "\n",
        "from tensorflow import keras\n",
        "\n",
        "print (tf.__version__)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TyKaGFr07yc0",
        "outputId": "b8081c38-6bcc-4265-eebc-846c6361c4db"
      },
      "source": [
        "recipes = pd.read_csv(\"RAW_recipes.csv\", usecols=['tags', 'steps'], nrows=10000)\n",
        "print (recipes)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                   tags                                              steps\n",
            "0     ['60-minutes-or-less', 'time-to-make', 'course...  ['make a choice and proceed with recipe', 'dep...\n",
            "1     ['30-minutes-or-less', 'time-to-make', 'course...  ['preheat oven to 425 degrees f', 'press dough...\n",
            "2     ['time-to-make', 'course', 'preparation', 'mai...  ['brown ground beef in large pot', 'add choppe...\n",
            "3     ['60-minutes-or-less', 'time-to-make', 'course...  ['place potatoes in a large pot of lightly sal...\n",
            "4     ['weeknight', 'time-to-make', 'course', 'main-...  ['mix all ingredients& boil for 2 1 / 2 hours ...\n",
            "...                                                 ...                                                ...\n",
            "9995  ['30-minutes-or-less', 'time-to-make', 'course...  ['mix lemon juice , vinegar , and country dijo...\n",
            "9996  ['15-minutes-or-less', 'time-to-make', 'course...  ['for sauce , combine in a bowl yogurt , mayon...\n",
            "9997  ['15-minutes-or-less', 'time-to-make', 'course...  ['in a skillet , bring enough water to cover a...\n",
            "9998  ['30-minutes-or-less', 'time-to-make', 'course...  ['mix parsley , garlic& oil in a 9 x 13 inch b...\n",
            "9999  ['15-minutes-or-less', 'time-to-make', 'course...  ['whisk mayo , yogurt , juice , zest and curry...\n",
            "\n",
            "[10000 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVYAWi-M9WVl"
      },
      "source": [
        "tags = recipes['tags']\n",
        "steps = recipes['steps']"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1n6xLre7Ajzd"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lSl9N9aA66m"
      },
      "source": [
        "def preprocess(sentence):\n",
        "  # Strip \"[]',\" from the sentence\n",
        "  sentence = sentence.translate(str.maketrans('', '', \"[]',\"))\n",
        "\n",
        "  # Adding a start and an end token to the sentence so that the model know when to start and stop predicting.\n",
        "  sentence = '<start> ' + sentence + ' <end>'\n",
        "\n",
        "  return sentence"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjpmI_wLOXsV"
      },
      "source": [
        "preprocessed_tags = ()\n",
        "for tag in tags:\n",
        "  tag = preprocess(tag)\n",
        "  preprocessed_tags += (tag,)\n",
        "\n",
        "preprocessed_steps = ()\n",
        "for step in steps:\n",
        "  step = preprocess(step)\n",
        "  preprocessed_steps += (step,)"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_vgaih1JWWS"
      },
      "source": [
        "#### Obtaining insights on lengths for defining maxlen"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8qquf3KJVh2"
      },
      "source": [
        "steps_lengths = pd.Series([len(x) for x in steps])\n",
        "tags_lengths = pd.Series([len(x) for x in tags])"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVNrnomyKAdF",
        "outputId": "2c39f9b0-ccef-4057-98f8-3084f4032777"
      },
      "source": [
        "steps_lengths.describe()"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    10000.000000\n",
              "mean       557.644200\n",
              "std        407.349132\n",
              "min          2.000000\n",
              "25%        294.000000\n",
              "50%        467.000000\n",
              "75%        704.250000\n",
              "max       5979.000000\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sYfAwOD4KDcF",
        "outputId": "66ee29fa-4bba-4279-ca84-75c60d0e4e86"
      },
      "source": [
        "tags_lengths.describe()"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    10000.000000\n",
              "mean       257.868900\n",
              "std        102.061412\n",
              "min          4.000000\n",
              "25%        182.000000\n",
              "50%        247.000000\n",
              "75%        321.000000\n",
              "max        706.000000\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAofD_MbKGzD"
      },
      "source": [
        "# maxlen\n",
        "# Taking values > and round figured to 75th percentile\n",
        "# At the same time not leaving high variance\n",
        "input_maxlen = 700\n",
        "target_maxlen = 350"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yyWH8zMeEns_"
      },
      "source": [
        "def tokenize(sentence, maxlen):\n",
        "  tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "      filters='')\n",
        "  tokenizer.fit_on_texts(sentence)\n",
        "\n",
        "  tensor = tokenizer.texts_to_sequences(sentence)\n",
        "\n",
        "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, maxlen=maxlen, padding='post', truncating='post')\n",
        "\n",
        "  return tensor, tokenizer"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Znns1wtkIlO5"
      },
      "source": [
        "input_tensor, inp_tokenizer = tokenize(preprocessed_steps, input_maxlen)\n",
        "target_tensor, targ_tokenizer = tokenize(preprocessed_tags, target_maxlen)"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4UhDdGaQrPb"
      },
      "source": [
        "with open('targ_tokenizer_cpu.pickle', 'wb') as handle:\n",
        "    pickle.dump(targ_tokenizer, handle)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kKC8FudKk-_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9ad02b6-b30c-4ba9-c8d6-71f3df747b80"
      },
      "source": [
        "BUFFER_SIZE = len(input_tensor)\n",
        "BATCH_SIZE = 64\n",
        "vocab_inp_size = len(inp_tokenizer.word_index)+1\n",
        "vocab_tar_size = len(targ_tokenizer.word_index)+1\n",
        "print (BUFFER_SIZE)\n",
        "print (vocab_inp_size)\n",
        "print (vocab_tar_size)\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor, target_tensor)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000\n",
            "13778\n",
            "478\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1daKsGdFP6n1"
      },
      "source": [
        "### Positional Encoding for adding notion of position among words as unlike RNN this is non-directional\n",
        "\n",
        "In a transformer, these positional encodings are passed to the encoder and decoder at the beginning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MvSiRbdiP7Uq"
      },
      "source": [
        "def get_angles(position, i, d_model):\n",
        "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
        "    \n",
        "    return position * angle_rates"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvctbIIwP8uE"
      },
      "source": [
        "def positional_encoding(position, d_model):\n",
        "    angle_rads = get_angles(\n",
        "        np.arange(position)[:, np.newaxis],\n",
        "        np.arange(d_model)[np.newaxis, :],\n",
        "        d_model\n",
        "    )\n",
        "\n",
        "    # Apply sin to even indices in the array; 2i\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "\n",
        "    # Apply cos to odd indices in the array; 2i+1\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "    pos_encoding = angle_rads[np.newaxis, ...]\n",
        "\n",
        "    return tf.cast(pos_encoding, dtype=tf.float32)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y37VruduQEza"
      },
      "source": [
        "### Masking\n",
        "\n",
        "- Padding mask for masking \"pad\" sequences\n",
        "- Lookahead mask for masking future words from contributing in prediction of current words in self attention\n",
        "\n",
        "The masking is applied to the decoding stage only."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Z3uml-XQAyI"
      },
      "source": [
        "def create_padding_mask(seq):\n",
        "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "    \n",
        "    return seq[:, tf.newaxis, tf.newaxis, :]"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQhLHTuAQHeO"
      },
      "source": [
        "def create_look_ahead_mask(size):\n",
        "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "    \n",
        "    return mask"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLvO7w8QQNBG"
      },
      "source": [
        "## Building the Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YN47Wg56QPAy"
      },
      "source": [
        "#### Scaled Dot Product"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xatJe5eqQJDZ"
      },
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "    matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
        "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "    if mask is not None:\n",
        "        scaled_attention_logits += (mask * -1e9)  \n",
        "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
        "    output = tf.matmul(attention_weights, v)\n",
        "\n",
        "    return output, attention_weights"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpjEId60QXWu"
      },
      "source": [
        "#### Multi-Headed Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdyCyMyyQUeM"
      },
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "\n",
        "        assert d_model % self.num_heads == 0\n",
        "\n",
        "        self.depth = d_model // self.num_heads\n",
        "\n",
        "        self.wq = tf.keras.layers.Dense(d_model)\n",
        "        self.wk = tf.keras.layers.Dense(d_model)\n",
        "        self.wv = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "        self.dense = tf.keras.layers.Dense(d_model)\n",
        "        \n",
        "    def split_heads(self, x, batch_size):\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        \n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "    \n",
        "    def call(self, v, k, q, mask):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "\n",
        "        q = self.wq(q)\n",
        "        k = self.wk(k)\n",
        "        v = self.wv(v)\n",
        "\n",
        "        q = self.split_heads(q, batch_size)\n",
        "        k = self.split_heads(k, batch_size)\n",
        "        v = self.split_heads(v, batch_size)\n",
        "\n",
        "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
        "            q, k, v, mask)\n",
        "\n",
        "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
        "\n",
        "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n",
        "        output = self.dense(concat_attention)\n",
        "            \n",
        "        return output, attention_weights"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qWLthaZQd7u"
      },
      "source": [
        "### Feed Forward Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1wzbuXiQfgf"
      },
      "source": [
        "def point_wise_feed_forward_network(d_model, dff): # dff is no of neurons in the layer\n",
        "    return tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(dff, activation='relu'),\n",
        "        tf.keras.layers.Dense(d_model)\n",
        "    ])"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5Zii0lkQjA0"
      },
      "source": [
        "#### Fundamental Unit of Transformer encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1sfBCtzQiEu"
      },
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "    def call(self, x, training, mask):\n",
        "        attn_output, _ = self.mha(x, x, x, mask)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(x + attn_output)\n",
        "\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        out2 = self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "        return out2"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHW9uB6nQtoA"
      },
      "source": [
        "#### Fundamental Unit of Transformer decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRCqhauBQk8a"
      },
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
        "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "    \n",
        "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
        "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)\n",
        "        attn1 = self.dropout1(attn1, training=training)\n",
        "        out1 = self.layernorm1(attn1 + x)\n",
        "\n",
        "        attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask)\n",
        "        attn2 = self.dropout2(attn2, training=training)\n",
        "        out2 = self.layernorm2(attn2 + out1)\n",
        "\n",
        "        ffn_output = self.ffn(out2)\n",
        "        ffn_output = self.dropout3(ffn_output, training=training)\n",
        "        out3 = self.layernorm3(ffn_output + out2)\n",
        "\n",
        "        return out3, attn_weights_block1, attn_weights_block2"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxiBodjmQxj1"
      },
      "source": [
        "#### Encoder consisting of multiple EncoderLayer(s)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLd9i2uLQyId"
      },
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, maximum_position_encoding, rate=0.1):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding, self.d_model)\n",
        "\n",
        "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
        "\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "        \n",
        "    def call(self, x, training, mask):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "\n",
        "        x = self.embedding(x)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "    \n",
        "        for i in range(self.num_layers):\n",
        "            x = self.enc_layers[i](x, training, mask)\n",
        "    \n",
        "        return x"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XF7Sue8VQ1Ju"
      },
      "source": [
        "#### Decoder consisting of multiple DecoderLayer(s)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQ_QLPY9Q118"
      },
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, maximum_position_encoding, rate=0.1):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
        "\n",
        "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        attention_weights = {}\n",
        "\n",
        "        x = self.embedding(x)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x, block1, block2 = self.dec_layers[i](x, enc_output, training, look_ahead_mask, padding_mask)\n",
        "\n",
        "            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
        "            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
        "    \n",
        "        return x, attention_weights"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWuvwELuQ3vB"
      },
      "source": [
        "#### Finally, the Transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OukAfQrnQ5N6"
      },
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, pe_input, pe_target, rate=0.1):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size, pe_input, rate)\n",
        "\n",
        "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size, pe_target, rate)\n",
        "\n",
        "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "    \n",
        "    def call(self, inputs, training):\n",
        "        inp, tar = inputs\n",
        "\n",
        "        enc_padding_mask, look_ahead_mask, dec_padding_mask  = self.create_masks(inp, tar)\n",
        "\n",
        "        enc_output = self.encoder(inp, training, enc_padding_mask)\n",
        "\n",
        "        dec_output, attention_weights = self.decoder(tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
        "\n",
        "        final_output = self.final_layer(dec_output)\n",
        "\n",
        "        return final_output, attention_weights\n",
        "\n",
        "    def create_masks(self, inp, tar):\n",
        "        # Encoder padding mask\n",
        "        enc_padding_mask = create_padding_mask(inp)\n",
        "\n",
        "        # Used in the 2nd attention block in the decoder.\n",
        "        # This padding mask is used to mask the encoder outputs.\n",
        "        dec_padding_mask = create_padding_mask(inp)\n",
        "\n",
        "        # Used in the 1st attention block in the decoder.\n",
        "        # It is used to pad and mask future tokens in the input received by\n",
        "        # the decoder.\n",
        "        look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
        "        dec_target_padding_mask = create_padding_mask(tar)\n",
        "        look_ahead_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
        "\n",
        "        return enc_padding_mask, look_ahead_mask, dec_padding_mask"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMi_pYWnREYq"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UxucuFY_RE2Z"
      },
      "source": [
        "# Hyper-params\n",
        "num_layers = 4\n",
        "d_model = 128\n",
        "dff = 512\n",
        "num_heads = 8\n",
        "EPOCHS = 10"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ra_YSp5wRLw1"
      },
      "source": [
        "#### Adam optimizer with custom learning rate scheduling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nb5I98qMRMuj"
      },
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(CustomSchedule, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "        self.warmup_steps = warmup_steps\n",
        "    \n",
        "    def __call__(self, step):\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5JBX_QqRPzw"
      },
      "source": [
        "#### Defining losses and other metrics "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_4LfeUDROl0"
      },
      "source": [
        "learning_rate = CustomSchedule(d_model)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gW0olpzKRS-k"
      },
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3TOEPadRUrR"
      },
      "source": [
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXDIVwzwRWDC"
      },
      "source": [
        "train_loss = tf.keras.metrics.Mean(name='train_loss')"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SaVStuyyRXee"
      },
      "source": [
        "#### Transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCQSsGlQRZJq"
      },
      "source": [
        "transformer = Transformer(\n",
        "    num_layers, \n",
        "    d_model, \n",
        "    num_heads, \n",
        "    dff,\n",
        "    vocab_inp_size, \n",
        "    vocab_tar_size, \n",
        "    pe_input=vocab_inp_size, \n",
        "    pe_target=vocab_tar_size,\n",
        ")"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtMQ9-aNReat"
      },
      "source": [
        "#### Checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgd8GMmhRfEq"
      },
      "source": [
        "checkpoint_path = \"checkpoints\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer, optimizer=optimizer)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "# if ckpt_manager.latest_checkpoint:\n",
        "#     ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "#     print ('Latest checkpoint restored!!') "
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nvx4LKveRh9C"
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, tar):\n",
        "    tar_inp = tar[:, :-1]\n",
        "    tar_real = tar[:, 1:]\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions, _ = transformer(\n",
        "            [inp, tar_inp], \n",
        "            True\n",
        "        )\n",
        "        loss = loss_function(tar_real, predictions)\n",
        "\n",
        "    gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
        "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "\n",
        "    train_loss(loss)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZtG6AeK4Rk1M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f26a5fa-b792-421f-b919-d3ec9f25d5dd"
      },
      "source": [
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    train_loss.reset_states()\n",
        "  \n",
        "    for (batch, (inp, tar)) in enumerate(dataset):\n",
        "        train_step(inp, tar)\n",
        "\n",
        "        if batch % 78 == 0:\n",
        "            print ('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch, train_loss.result()))\n",
        "      \n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        ckpt_save_path = ckpt_manager.save()\n",
        "        print ('Saving checkpoint for epoch {} at {}'.format(epoch+1, ckpt_save_path))\n",
        "    \n",
        "    print ('Epoch {} Loss {:.4f}'.format(epoch + 1, train_loss.result()))\n",
        "\n",
        "    print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 6.2951\n",
            "Epoch 1 Loss 6.2708\n",
            "Time taken for 1 epoch: 681.9398462772369 secs\n",
            "\n",
            "Epoch 2 Batch 0 Loss 6.2405\n",
            "Epoch 2 Loss 6.1051\n",
            "Time taken for 1 epoch: 608.4823083877563 secs\n",
            "\n",
            "Epoch 3 Batch 0 Loss 5.9638\n",
            "Epoch 3 Loss 5.8233\n",
            "Time taken for 1 epoch: 605.0800099372864 secs\n",
            "\n",
            "Epoch 4 Batch 0 Loss 5.7017\n",
            "Epoch 4 Loss 5.5679\n",
            "Time taken for 1 epoch: 612.1189675331116 secs\n",
            "\n",
            "Epoch 5 Batch 0 Loss 5.4553\n",
            "Saving checkpoint for epoch 5 at checkpoints/ckpt-1\n",
            "Epoch 5 Loss 5.3906\n",
            "Time taken for 1 epoch: 615.1195859909058 secs\n",
            "\n",
            "Epoch 6 Batch 0 Loss 5.3439\n",
            "Epoch 6 Loss 5.2717\n",
            "Time taken for 1 epoch: 610.9490187168121 secs\n",
            "\n",
            "Epoch 7 Batch 0 Loss 5.2018\n",
            "Epoch 7 Loss 5.1640\n",
            "Time taken for 1 epoch: 607.4905774593353 secs\n",
            "\n",
            "Epoch 8 Batch 0 Loss 5.1542\n",
            "Epoch 8 Loss 5.0605\n",
            "Time taken for 1 epoch: 619.1171875 secs\n",
            "\n",
            "Epoch 9 Batch 0 Loss 4.9730\n",
            "Epoch 9 Loss 4.9003\n",
            "Time taken for 1 epoch: 612.0162568092346 secs\n",
            "\n",
            "Epoch 10 Batch 0 Loss 4.8009\n",
            "Saving checkpoint for epoch 10 at checkpoints/ckpt-2\n",
            "Epoch 10 Loss 4.7030\n",
            "Time taken for 1 epoch: 613.7543151378632 secs\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piBh5NKwO_tA"
      },
      "source": [
        "### Inference\n",
        "Predicting one word at a time at the decoder and appending it to the output; then taking the complete sequence as an input to the decoder and repeating until maxlen or stop keyword appears"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89Jk1T43jMDk"
      },
      "source": [
        "class GenerateTags(tf.Module):\n",
        "    def __init__(self, inp_tokenizer, targ_tokenizer, transformer):\n",
        "        super(GenerateTags, self).__init__()\n",
        "        self.inp_tokenizer = inp_tokenizer\n",
        "        self.targ_tokenizer = targ_tokenizer\n",
        "        self.transformer = transformer\n",
        "\n",
        "    def __call__(self, input_recipe, input_maxlen=700, target_maxlen=350):\n",
        "        input_recipe = self.inp_tokenizer.texts_to_sequences([input_recipe])\n",
        "        input_recipe = tf.keras.preprocessing.sequence.pad_sequences(input_recipe, maxlen=input_maxlen, padding='post', truncating='post')\n",
        "\n",
        "        encoder_input = tf.expand_dims(input_recipe[0], 0)\n",
        "\n",
        "        decoder_input = [self.targ_tokenizer.word_index[\"<start>\"]]\n",
        "        output = tf.expand_dims(decoder_input, 0)\n",
        "        \n",
        "        for i in range(target_maxlen):\n",
        "            predictions, attention_weights = self.transformer(\n",
        "                [encoder_input, output],\n",
        "                False\n",
        "            )\n",
        "\n",
        "            predictions = predictions[: ,-1:, :]\n",
        "            predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
        "\n",
        "            if predicted_id == self.targ_tokenizer.word_index[\"<end>\"]:\n",
        "                break\n",
        "            else:\n",
        "                output = tf.concat([output, predicted_id], axis=-1)\n",
        "\n",
        "        tags = tf.squeeze(output, axis=0)\n",
        "        tags = tags.numpy()\n",
        "        tags = np.expand_dims(tags[1:], 0)\n",
        "\n",
        "        return self.targ_tokenizer.sequences_to_texts(tags)[0]"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfCzD475xrWx"
      },
      "source": [
        "generate_tags = GenerateTags(inp_tokenizer, targ_tokenizer, transformer)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRmmZjKkiWK0"
      },
      "source": [
        "# Save transformer weights\n",
        "transformer.save_weights('transformer')"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UUxTOY8si39g",
        "outputId": "6ee75ffc-93fa-4727-910a-0982bf10790f"
      },
      "source": [
        "# Load transformer weights\n",
        "# transformer.load_weights('transformer')"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f7ff81c30d0>"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOXLVOcZMydx"
      },
      "source": [
        "sentence = \"'make a choice and proceed with recipe', 'depending on size of squash , cut into half or fourths', 'remove seeds', 'for spicy squash , drizzle olive oil or melted butter over each cut squash piece', 'season with mexican seasoning mix ii', 'for sweet squash , drizzle melted honey , butter , grated piloncillo over each cut squash piece', 'season with sweet mexican spice mix', 'bake at 350 degrees , again depending on size , for 40 minutes up to an hour , until a fork can easily pierce the skin', 'be careful not to burn the squash especially if you opt to use sugar or butter', 'if you feel more comfortable , cover the squash with aluminum foil the first half hour , give or take , of baking', 'if desired , season with salt'\"\n",
        "sentence = sentence.translate(str.maketrans('', '', \"[]',\"))"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhy1hEWex6Ah",
        "outputId": "60282a79-e330-44f9-ff10-4901f795ba5f"
      },
      "source": [
        "tags = generate_tags(sentence)\n",
        "print (tags)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time-to-make time-to-make course preparation preparation preparation dietary\n"
          ]
        }
      ]
    }
  ]
}
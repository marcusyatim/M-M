##### exp1:

- Added a few stopwords (refer to ./stopwords/exp1.txt).
- Removed all punctuations, numbers and tokens with word length <3.
- text = text.replace("-","") is used to remove time stamps, e.g. 15-20 minutes, because the tokenisation separates
  '15-20' as a token and then this cannot be captured under string.punctuation and isnumeric(). (Or should we leave this in?)

def pre_process(text):
    text = text.replace("'","")
    text = text.replace("-","")
    tokens = nltk.word_tokenize(text)
    tokens = [ t for t in tokens if t not in string.punctuation+"’“”'" ]
    tokens = [ t for t in tokens if not t.isnumeric() ]
    tokens = [ WNlemma.lemmatize(t.lower()) for t in tokens ]
    tokens = [ t for t in tokens if t not in mystopwords ]
    tokens = [ t for t in tokens if len(t) >= 3 ]
    return(tokens)

- Filter off any words with document frequency less than 2, or appearing in more than 80% documents.
- Too low?
dictionary.filter_extremes(no_below=2, no_above=0.8)

Dictionary(50614 unique tokens: ['aluminum', 'bake', 'baking', 'burn', 'butter']...)
Dictionary(22035 unique tokens: ['aluminum', 'bake', 'baking', 'burn', 'butter']...)

- 25 topics. Too many?
lda = gensim.models.ldamodel.LdaModel(dtm, num_topics=25, id2word=dictionary, passes=10, chunksize=128, random_state=10)

'''
u_mass:prefer the model close to 0. 
c_v: [0,1], prefer bigger value.   
Do not fully rely on the coherence score.
'''
print(lda_umass)
print(lda_cv)
-2.2833287487837315
0.43888768907805975

Topics: Not Considered

Comments: Not enough pre-processing done.

##### exp2:

- No additional stopwords.
- Removed all punctuations and tokens with word length. 
- Numbers remain.

def pre_process(text):
    text = text.replace("'","")
    tokens = nltk.word_tokenize(text)
    tokens = [ t for t in tokens if t not in string.punctuation+"’“”'" ]
    tokens = [ WNlemma.lemmatize(t.lower()) for t in tokens ]
    tokens = [ t for t in tokens if t not in mystopwords ]
    tokens = [ t for t in tokens if len(t) >= 3 ]
    return(tokens)

- Filter off any words with document frequency less than 100, or appearing in more than 70% documents.
- Considering there are over 200k recipes, should filter tf-idf more? Or too much?
dictionary.filter_extremes(no_below=100, no_above=0.7)

Dictionary(53776 unique tokens: ['350', 'aluminum', 'bake', 'baking', 'burn']...)
Dictionary(3072 unique tokens: ['350', 'aluminum', 'bake', 'baking', 'burn']...)

- 15 topics. Too little?
lda = gensim.models.ldamodel.LdaModel(dtm, num_topics=15, id2word=dictionary, passes=10, chunksize=128, random_state=10)

'''
u_mass:prefer the model close to 0. 
c_v: [0,1], prefer bigger value.   
Do not fully rely on the coherence score.
'''
print(lda_umass)
print(lda_cv)
-1.9039208355178339
0.4945125297358912

Topics:
1) Main course, chicken
2) Oven baked, main course
3) Boiled, appetisers
4) Baking, pastry, dessert
5) Vegetables
6) Meat, pasta
7) Cheesy bread, pizza, sandwich
8) Recipe steps???
9) Salad
10) Dough, tortila, baking
11) Potato, roast
12) Blended, smoothies
13) Microwave, food to keep
14) Grilled, steaks, bbq, burger
15) Turkey, marinate, seasonal/holiday feast

Comments: Data preprocessing seems good, even though no stopwords used.
Topics seems alright with the exception of topic 8.

##### exp3:

- Added in lots of stopwords. Mostly to account for the weird topic 8 from exp2.
- All other experiment designs remain similar to exp2.

Dictionary(53753 unique tokens: ['350', 'aluminum', 'bake', 'baking', 'burn']...)
Dictionary(3049 unique tokens: ['350', 'aluminum', 'bake', 'baking', 'burn']...)

'''
u_mass:prefer the model close to 0. 
c_v: [0,1], prefer bigger value.   
Do not fully rely on the coherence score.
'''
print(lda_umass)
print(lda_cv)
-2.240728339917223
0.47745641348478673

Topics:
1) Pasta, Italian
2) Main course, chicken, beef, fish
3) Baking, cake
4) Baking, bread
5) Baking, pastry
6) Oven baked, dishes
7) Blended, smoothies, yoghurt, fruits, salad
8) Cheesy, tomato, dishes
9) Vegetable dishes
10) Tortilla, salsa
11) Citrus baked goodies
12) Potato based dishes
13) Meat dishes
14) Glazed ham???
15) Burger, bun, patty

Comments: Really like the topics extracted, quite easy to defined. Except for topic 14.

##### exp4:

- Added in more stopwords. Mostly to account for the weird topic 14 from exp3.
- All other experiment designs remain similar to exp2,3.

Dictionary(53735 unique tokens: ['350', 'aluminum', 'bake', 'baking', 'burn']...)
Dictionary(3031 unique tokens: ['350', 'aluminum', 'bake', 'baking', 'burn']...)

'''
u_mass:prefer the model close to 0. 
c_v: [0,1], prefer bigger value.   
Do not fully rely on the coherence score.
'''
print(lda_umass)
print(lda_cv)
-2.0832374492307606
0.48976501475294776

Topics:
1) General cooking methods
2) Baking, sweets
3) Baking, dough
4) Baking, cheesy dishes
5) Roasts, vegetables, main course
6) Oven-baked dishes
7) Rice dishes, chicken, fish
8) Cold dishes, instant meals
9) Salad
10) Citrus drinks, desserts
11) Pasta
12) Burrito, bean, beef, tortila
13) Pork chops
14) Marinates
15) Potato based dishes

Comments: Even though topics have proper Top-N words, but hard to define them and topics
are quite vague.